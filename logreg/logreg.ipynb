{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and SGD Homework \n",
    "***\n",
    "**Name**: Akriti Kapur \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 9th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "In this homework you'll implement stochastic gradient ascent for logistic regression and you'll apply it to the task of determining whether documents are talking about automobiles or motorcycles.\n",
    "\n",
    "<br>\n",
    "\n",
    "![autos_motorcycles](autos_motorcycles.jpg \"A car and a motorcycle\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "You should not use any libraries that implement any of the functionality of logistic regression for this assignment; logistic regression is implemented in Scikit-Learn, but you should do everything by hand now. You'll be able to use library implementations of logistic regression in the future.\n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1: Loading and Exploring the Data\n",
    "***\n",
    "\n",
    "The `Example` class will be used to store the features and labels associated with a single training or test example.  The `read_data` function will read in the text data and split it into training and test sets.  \n",
    "\n",
    " Load the data and then do the following: \n",
    "- Report the number of words in the vocabulary \n",
    "- Explain how the code is creating features (i.e. what text model is being used). \n",
    "- Go into the raw text files in the data directory and figure out which label (0/1) refers to which class of document (automobiles or motorcycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kSEED = 1735\n",
    "kBIAS = \"BIAS_CONSTANT\"\n",
    "\n",
    "np.random.seed(kSEED)\n",
    "\n",
    "class Example:\n",
    "    \"\"\"\n",
    "    Class to represent a document example\n",
    "    \"\"\"\n",
    "    def __init__(self, label, words, vocab):\n",
    "        \"\"\"\n",
    "        Create a new example\n",
    "\n",
    "        :param label: The label (0 / 1) of the example\n",
    "        :param words: The words in a list of \"word:count\" format\n",
    "        :param vocab: The vocabulary to use as features (list)\n",
    "        \"\"\"\n",
    "        self.nonzero = {}\n",
    "        self.y = label\n",
    "        self.x = np.zeros(len(vocab))\n",
    "        for word, count in [x.split(\":\") for x in words]:\n",
    "            if word in vocab:\n",
    "                assert word != kBIAS, \"Bias can't actually appear in document\"\n",
    "                self.x[vocab.index(word)] += float(count)\n",
    "                self.nonzero[vocab.index(word)] = word\n",
    "        self.x[0] = 1\n",
    "\n",
    "def read_dataset(positive, negative, vocab, train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Reads in a text dataset with a given vocabulary\n",
    "\n",
    "    :param positive: Positive examples\n",
    "    :param negative: Negative examples\n",
    "    :param vocab: A list of vocabulary words\n",
    "    :param test_frac: How much of the data should be reserved for test\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = [x.split(\"\\t\")[0] for x in open(vocab, 'r') if '\\t' in x]\n",
    "    assert vocab[0] == kBIAS, \\\n",
    "        \"First vocab word must be bias term (was %s)\" % vocab[0]\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for label, input in [(1, positive), (0, negative)]:\n",
    "        for line in open(input):\n",
    "            ex = Example(label, line.split(), vocab)\n",
    "            if np.random.random() <= train_frac:\n",
    "                train_set.append(ex)\n",
    "            else:\n",
    "                test_set.append(ex)\n",
    "\n",
    "    # Shuffle the data \n",
    "    np.random.shuffle(train_set)\n",
    "    np.random.shuffle(test_set)\n",
    "\n",
    "    return train_set, test_set, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_fname = \"../data/autos_motorcycles/positive\"\n",
    "neg_fname = \"../data/autos_motorcycles/negative\"\n",
    "voc_fname = \"../data/autos_motorcycles/vocab\"\n",
    "train_set, test_set, vocab = read_dataset(pos_fname, neg_fname, voc_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary - 5327\n",
      "The number of features are the length of the vocabulary.For each sentence in the positive and negative files (which already has the count of uinque words),the features are words, where the index of the word in the feature list (vocab list) containsthe number of times that word occurs in the sentence.\n",
      "Motorcycle has a label 1 , Automobile - 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of words in the vocabulary - {}'.format(len(vocab)))\n",
    "print('The number of features are the length of the vocabulary.' + \n",
    "      'For each sentence in the positive and negative files (which already has the count of uinque words),' +\n",
    "      'the features are words, where the index of the word in the feature list (vocab list) contains' + \n",
    "      'the number of times that word occurs in the sentence.')\n",
    "print('Motorcycle has a label 1 , Automobile - 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Implementing SGD with Lazy Sparse Regularization\n",
    "***\n",
    "\n",
    "We've given you a class `LogReg` below which will train a logistic regression classifier to predict whether a document is talking about automobiles or motorcycles. \n",
    "\n",
    "**Part A**: In this problem you will modify the `sgd_update` function to perform **unregularized** stochastic gradient descent updates of the weights. Note that you should only update the weights for **non-zero** features, i.e. weights associated with words that appear in the current training example. The code below this cell demonstrates how to instantiate the class and train the classifier.   \n",
    "\n",
    "We've also given you unit tests in the next cell based on the simple example worked out in  the Lecture 4 in-class notebook.  At first your code will fail both of them. When your code is working you should pass tests called `test_unreg` and `test_learnrate`.  Do not move on to **Part A** until your code passes both of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    def __init__(self, train_set, test_set, lam, eta=0.1):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "\n",
    "        :param train_set: A set of training examples\n",
    "        :param test_set: A set of test examples \n",
    "        :param lam: Regularization parameter\n",
    "        :param eta: The learning rate to use \n",
    "        \"\"\"\n",
    "        \n",
    "        # Store training and test sets \n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set \n",
    "        \n",
    "        # Initialize vector of weights to zero  \n",
    "        self.w = np.zeros_like(train_set[0].x)\n",
    "        \n",
    "        # Store regularization parameter and eta function \n",
    "        self.lam = lam\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Create dictionary for lazy-sparse regularization\n",
    "        self.last_update = dict()\n",
    "\n",
    "        # Make sure regularization parameter is not negative \n",
    "        assert self.lam>= 0, \"Regularization parameter must be non-negative\"\n",
    "        \n",
    "        # Empty lists to store NLL and accuracy on train and test sets \n",
    "        self.train_nll = []\n",
    "        self.test_nll = []\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "        # dict to store the last updated iteration of feature\n",
    "        self.memory = {}\n",
    "        \n",
    "    def sigmoid(self,score, threshold=20.0):\n",
    "        \"\"\"\n",
    "        Prevent overflow of exp by capping activation at 20.\n",
    "        You do not need to change this function. \n",
    "\n",
    "        :param score: A real valued number to convert into a number between 0 and 1\n",
    "        \"\"\"\n",
    "\n",
    "        # if score > threshold, cap value at score \n",
    "        if abs(score) > threshold:\n",
    "            score = threshold * np.sign(score)\n",
    "\n",
    "        return 1.0 / (1.0 + np.exp(-score)) \n",
    "\n",
    "    def compute_progress(self, examples):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the NLL and accuracy\n",
    "        You shouldn't need to change this function. \n",
    "\n",
    "        :param examples: The dataset to score\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        NLL = 0.0\n",
    "        num_correct = 0\n",
    "        for ex in examples:\n",
    "            # compute prob prediction\n",
    "            p = self.sigmoid(self.w.dot(ex.x))\n",
    "            # update negative log likelihood\n",
    "            NLL = NLL - np.log(p) if ex.y==1 else NLL - np.log(1.0-p)\n",
    "            # update number correct \n",
    "            num_correct += 1 if np.floor(p+.5)==ex.y else 0\n",
    "\n",
    "        return NLL, float(num_correct) / float(len(examples))\n",
    "    \n",
    "    def train(self, num_epochs=1, isVerbose=False, report_step=5):\n",
    "        \"\"\"\n",
    "        Train the logistic regression classifier on the training data \n",
    "\n",
    "        :param num_epochs: number of full passes over data to perform \n",
    "        :param isVerbose: boolean indicating whether to print progress\n",
    "        :param report_step: how many iterations between recording progress\n",
    "        \"\"\"\n",
    "        iteration = 0\n",
    "        # Perform an epoch \n",
    "        for pp in range(num_epochs):\n",
    "            # shuffle the data  \n",
    "            np.random.shuffle(self.train_set)\n",
    "            # loop over each training example\n",
    "            for ex in self.train_set:\n",
    "                # perform SGD update of weights \n",
    "                self.sgd_update(ex, iteration)\n",
    "                # record progress \n",
    "                if iteration % report_step == 1:\n",
    "                    train_nll, train_acc = self.compute_progress(self.train_set)\n",
    "                    test_nll, test_acc = self.compute_progress(self.test_set)\n",
    "                    self.train_nll.append(train_nll)\n",
    "                    self.test_nll.append(test_nll)\n",
    "                    self.train_acc.append(train_acc)\n",
    "                    self.test_acc.append(test_acc)\n",
    "                    if isVerbose:\n",
    "                        print(\"Update {: 5d}  TrnNLL {: 8.3f}  TstNLL {: 8.3f}  TrnA {:.3f}  TstA {:.3f}\"\n",
    "                             .format(iteration-1, train_nll, test_nll, train_acc, test_acc))\n",
    "                iteration += 1\n",
    "    \n",
    "    def sgd_update(self, train_example, iteration):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the NLL \n",
    "\n",
    "        :param train_example: The example to take the gradient with respect to\n",
    "        :param iteration: The current iteration (an integer)\n",
    "        \"\"\"\n",
    "        muu = (self.sigmoid(np.dot(self.w, train_example.x)) - train_example.y)\n",
    "        dw = muu * train_example.x\n",
    "        \n",
    "        # shrinkage factor\n",
    "        shrinkage_factor = 1 - 2 * self.eta * self.lam\n",
    "\n",
    "        # Unregularized update\n",
    "        self.w = self.w - self.eta * dw\n",
    "    \n",
    "        # Assign prior value to feature if index not present in dictionary.\n",
    "        for feature in range(1, len(train_example.x)):\n",
    "            if not feature in self.memory:\n",
    "                self.memory[feature] = -1\n",
    "        \n",
    "        for index in range(1, len(train_example.x)):\n",
    "            if train_example.x[index] != 0:\n",
    "                # if x is non-zero, perform update to weight.\n",
    "                self.w[index] = self.w[index] * pow(shrinkage_factor, iteration - self.memory[index])\n",
    "                # update last modified iteration for feature.\n",
    "                self.memory[index] = iteration\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update     0  TrnNLL  880.873  TstNLL   87.017  TrnA 0.498  TstA 0.534\n",
      "Update     5  TrnNLL  587.666  TstNLL   62.668  TrnA 0.706  TstA 0.724\n",
      "Update    10  TrnNLL  630.383  TstNLL   66.444  TrnA 0.682  TstA 0.707\n",
      "Update    15  TrnNLL  559.246  TstNLL   58.871  TrnA 0.718  TstA 0.767\n",
      "Update    20  TrnNLL  542.703  TstNLL   55.050  TrnA 0.756  TstA 0.741\n",
      "Update    25  TrnNLL  515.403  TstNLL   54.344  TrnA 0.772  TstA 0.750\n",
      "Update    30  TrnNLL  502.789  TstNLL   53.166  TrnA 0.777  TstA 0.784\n",
      "Update    35  TrnNLL  685.808  TstNLL   72.709  TrnA 0.651  TstA 0.672\n",
      "Update    40  TrnNLL  512.530  TstNLL   54.816  TrnA 0.754  TstA 0.759\n",
      "Update    45  TrnNLL  455.235  TstNLL   45.167  TrnA 0.797  TstA 0.845\n",
      "Update    50  TrnNLL  412.981  TstNLL   40.363  TrnA 0.839  TstA 0.879\n",
      "Update    55  TrnNLL  408.755  TstNLL   41.873  TrnA 0.841  TstA 0.828\n",
      "Update    60  TrnNLL  387.590  TstNLL   39.614  TrnA 0.858  TstA 0.879\n",
      "Update    65  TrnNLL  364.139  TstNLL   38.807  TrnA 0.866  TstA 0.879\n",
      "Update    70  TrnNLL  360.882  TstNLL   39.067  TrnA 0.861  TstA 0.836\n",
      "Update    75  TrnNLL  433.364  TstNLL   49.634  TrnA 0.821  TstA 0.828\n",
      "Update    80  TrnNLL  432.043  TstNLL   50.021  TrnA 0.818  TstA 0.828\n",
      "Update    85  TrnNLL  384.933  TstNLL   45.309  TrnA 0.850  TstA 0.836\n",
      "Update    90  TrnNLL  368.728  TstNLL   43.434  TrnA 0.859  TstA 0.853\n",
      "Update    95  TrnNLL  373.923  TstNLL   44.162  TrnA 0.858  TstA 0.853\n",
      "Update   100  TrnNLL  369.048  TstNLL   44.019  TrnA 0.853  TstA 0.845\n",
      "Update   105  TrnNLL  345.667  TstNLL   40.896  TrnA 0.871  TstA 0.845\n",
      "Update   110  TrnNLL  375.815  TstNLL   56.824  TrnA 0.859  TstA 0.810\n",
      "Update   115  TrnNLL  316.434  TstNLL   39.704  TrnA 0.880  TstA 0.828\n",
      "Update   120  TrnNLL  303.508  TstNLL   36.533  TrnA 0.884  TstA 0.853\n",
      "Update   125  TrnNLL  289.590  TstNLL   34.439  TrnA 0.886  TstA 0.879\n",
      "Update   130  TrnNLL  279.562  TstNLL   33.963  TrnA 0.907  TstA 0.862\n",
      "Update   135  TrnNLL  280.717  TstNLL   33.552  TrnA 0.895  TstA 0.897\n",
      "Update   140  TrnNLL  274.778  TstNLL   34.888  TrnA 0.908  TstA 0.879\n",
      "Update   145  TrnNLL  271.841  TstNLL   34.357  TrnA 0.909  TstA 0.879\n",
      "Update   150  TrnNLL  290.834  TstNLL   38.128  TrnA 0.903  TstA 0.871\n",
      "Update   155  TrnNLL  275.292  TstNLL   34.700  TrnA 0.908  TstA 0.905\n",
      "Update   160  TrnNLL  262.232  TstNLL   31.173  TrnA 0.908  TstA 0.897\n",
      "Update   165  TrnNLL  260.694  TstNLL   30.554  TrnA 0.908  TstA 0.905\n",
      "Update   170  TrnNLL  255.925  TstNLL   29.797  TrnA 0.910  TstA 0.905\n",
      "Update   175  TrnNLL  256.193  TstNLL   30.940  TrnA 0.913  TstA 0.905\n",
      "Update   180  TrnNLL  256.716  TstNLL   31.603  TrnA 0.913  TstA 0.897\n",
      "Update   185  TrnNLL  259.893  TstNLL   32.590  TrnA 0.909  TstA 0.862\n",
      "Update   190  TrnNLL  251.606  TstNLL   30.717  TrnA 0.915  TstA 0.879\n",
      "Update   195  TrnNLL  248.805  TstNLL   27.815  TrnA 0.913  TstA 0.922\n",
      "Update   200  TrnNLL  245.254  TstNLL   27.945  TrnA 0.915  TstA 0.922\n",
      "Update   205  TrnNLL  246.780  TstNLL   27.873  TrnA 0.911  TstA 0.922\n",
      "Update   210  TrnNLL  251.211  TstNLL   31.229  TrnA 0.916  TstA 0.888\n",
      "Update   215  TrnNLL  232.136  TstNLL   24.905  TrnA 0.923  TstA 0.914\n",
      "Update   220  TrnNLL  231.662  TstNLL   24.741  TrnA 0.924  TstA 0.914\n",
      "Update   225  TrnNLL  232.467  TstNLL   24.040  TrnA 0.924  TstA 0.922\n",
      "Update   230  TrnNLL  242.093  TstNLL   24.700  TrnA 0.917  TstA 0.922\n",
      "Update   235  TrnNLL  240.897  TstNLL   24.542  TrnA 0.917  TstA 0.922\n",
      "Update   240  TrnNLL  221.623  TstNLL   22.343  TrnA 0.928  TstA 0.922\n",
      "Update   245  TrnNLL  221.201  TstNLL   21.958  TrnA 0.931  TstA 0.914\n",
      "Update   250  TrnNLL  216.140  TstNLL   22.806  TrnA 0.934  TstA 0.914\n",
      "Update   255  TrnNLL  220.346  TstNLL   24.833  TrnA 0.932  TstA 0.888\n",
      "Update   260  TrnNLL  222.799  TstNLL   25.525  TrnA 0.928  TstA 0.888\n",
      "Update   265  TrnNLL  217.088  TstNLL   23.229  TrnA 0.931  TstA 0.897\n",
      "Update   270  TrnNLL  215.549  TstNLL   22.683  TrnA 0.933  TstA 0.897\n",
      "Update   275  TrnNLL  211.865  TstNLL   22.217  TrnA 0.932  TstA 0.905\n",
      "Update   280  TrnNLL  215.888  TstNLL   21.990  TrnA 0.930  TstA 0.931\n",
      "Update   285  TrnNLL  201.006  TstNLL   21.141  TrnA 0.941  TstA 0.922\n",
      "Update   290  TrnNLL  199.762  TstNLL   20.630  TrnA 0.939  TstA 0.922\n",
      "Update   295  TrnNLL  197.797  TstNLL   20.738  TrnA 0.939  TstA 0.922\n",
      "Update   300  TrnNLL  194.261  TstNLL   21.436  TrnA 0.938  TstA 0.914\n",
      "Update   305  TrnNLL  194.325  TstNLL   21.244  TrnA 0.936  TstA 0.914\n",
      "Update   310  TrnNLL  192.725  TstNLL   21.722  TrnA 0.941  TstA 0.897\n",
      "Update   315  TrnNLL  195.770  TstNLL   21.344  TrnA 0.941  TstA 0.922\n",
      "Update   320  TrnNLL  195.725  TstNLL   21.179  TrnA 0.940  TstA 0.922\n",
      "Update   325  TrnNLL  196.056  TstNLL   20.748  TrnA 0.939  TstA 0.922\n",
      "Update   330  TrnNLL  192.989  TstNLL   20.163  TrnA 0.939  TstA 0.931\n",
      "Update   335  TrnNLL  188.532  TstNLL   20.389  TrnA 0.941  TstA 0.922\n",
      "Update   340  TrnNLL  184.990  TstNLL   20.018  TrnA 0.944  TstA 0.922\n",
      "Update   345  TrnNLL  180.950  TstNLL   19.507  TrnA 0.944  TstA 0.922\n",
      "Update   350  TrnNLL  177.279  TstNLL   20.255  TrnA 0.943  TstA 0.897\n",
      "Update   355  TrnNLL  176.710  TstNLL   20.127  TrnA 0.943  TstA 0.897\n",
      "Update   360  TrnNLL  179.151  TstNLL   21.015  TrnA 0.939  TstA 0.897\n",
      "Update   365  TrnNLL  177.345  TstNLL   20.510  TrnA 0.941  TstA 0.897\n",
      "Update   370  TrnNLL  174.730  TstNLL   20.113  TrnA 0.943  TstA 0.905\n",
      "Update   375  TrnNLL  152.603  TstNLL   18.980  TrnA 0.948  TstA 0.914\n",
      "Update   380  TrnNLL  152.422  TstNLL   17.474  TrnA 0.950  TstA 0.922\n",
      "Update   385  TrnNLL  151.560  TstNLL   17.426  TrnA 0.948  TstA 0.922\n",
      "Update   390  TrnNLL  151.553  TstNLL   17.258  TrnA 0.951  TstA 0.940\n",
      "Update   395  TrnNLL  151.138  TstNLL   17.209  TrnA 0.951  TstA 0.940\n",
      "Update   400  TrnNLL  148.053  TstNLL   17.006  TrnA 0.954  TstA 0.940\n",
      "Update   405  TrnNLL  150.191  TstNLL   16.687  TrnA 0.952  TstA 0.940\n",
      "Update   410  TrnNLL  154.718  TstNLL   17.123  TrnA 0.951  TstA 0.940\n",
      "Update   415  TrnNLL  152.708  TstNLL   17.180  TrnA 0.949  TstA 0.940\n",
      "Update   420  TrnNLL  152.684  TstNLL   17.201  TrnA 0.949  TstA 0.940\n",
      "Update   425  TrnNLL  157.454  TstNLL   17.442  TrnA 0.946  TstA 0.940\n",
      "Update   430  TrnNLL  147.158  TstNLL   16.356  TrnA 0.955  TstA 0.948\n",
      "Update   435  TrnNLL  150.011  TstNLL   16.595  TrnA 0.949  TstA 0.948\n",
      "Update   440  TrnNLL  149.137  TstNLL   16.565  TrnA 0.951  TstA 0.948\n",
      "Update   445  TrnNLL  138.266  TstNLL   16.610  TrnA 0.958  TstA 0.948\n",
      "Update   450  TrnNLL  138.237  TstNLL   16.609  TrnA 0.958  TstA 0.948\n",
      "Update   455  TrnNLL  138.273  TstNLL   16.540  TrnA 0.958  TstA 0.948\n",
      "Update   460  TrnNLL  137.257  TstNLL   16.071  TrnA 0.958  TstA 0.948\n",
      "Update   465  TrnNLL  137.023  TstNLL   16.025  TrnA 0.958  TstA 0.948\n",
      "Update   470  TrnNLL  135.299  TstNLL   16.187  TrnA 0.959  TstA 0.940\n",
      "Update   475  TrnNLL  134.908  TstNLL   16.140  TrnA 0.959  TstA 0.940\n",
      "Update   480  TrnNLL  134.591  TstNLL   16.187  TrnA 0.959  TstA 0.940\n",
      "Update   485  TrnNLL  132.825  TstNLL   16.320  TrnA 0.961  TstA 0.931\n",
      "Update   490  TrnNLL  134.752  TstNLL   16.278  TrnA 0.959  TstA 0.940\n",
      "Update   495  TrnNLL  132.939  TstNLL   16.006  TrnA 0.960  TstA 0.948\n",
      "Update   500  TrnNLL  134.134  TstNLL   16.715  TrnA 0.958  TstA 0.931\n",
      "Update   505  TrnNLL  133.375  TstNLL   16.127  TrnA 0.957  TstA 0.931\n",
      "Update   510  TrnNLL  132.930  TstNLL   16.014  TrnA 0.957  TstA 0.931\n",
      "Update   515  TrnNLL  131.601  TstNLL   15.681  TrnA 0.957  TstA 0.940\n",
      "Update   520  TrnNLL  133.330  TstNLL   16.521  TrnA 0.957  TstA 0.931\n",
      "Update   525  TrnNLL  135.564  TstNLL   17.028  TrnA 0.959  TstA 0.931\n",
      "Update   530  TrnNLL  130.855  TstNLL   15.536  TrnA 0.959  TstA 0.948\n",
      "Update   535  TrnNLL  129.202  TstNLL   15.323  TrnA 0.962  TstA 0.940\n",
      "Update   540  TrnNLL  129.232  TstNLL   16.373  TrnA 0.958  TstA 0.931\n",
      "Update   545  TrnNLL  128.523  TstNLL   16.197  TrnA 0.958  TstA 0.931\n",
      "Update   550  TrnNLL  142.016  TstNLL   20.742  TrnA 0.958  TstA 0.922\n",
      "Update   555  TrnNLL  149.421  TstNLL   18.484  TrnA 0.954  TstA 0.931\n",
      "Update   560  TrnNLL  152.379  TstNLL   18.803  TrnA 0.953  TstA 0.931\n",
      "Update   565  TrnNLL  137.247  TstNLL   20.689  TrnA 0.958  TstA 0.922\n",
      "Update   570  TrnNLL  135.927  TstNLL   20.216  TrnA 0.957  TstA 0.931\n",
      "Update   575  TrnNLL  131.995  TstNLL   18.504  TrnA 0.961  TstA 0.922\n",
      "Update   580  TrnNLL  131.632  TstNLL   18.536  TrnA 0.960  TstA 0.922\n",
      "Update   585  TrnNLL  129.538  TstNLL   18.630  TrnA 0.964  TstA 0.922\n",
      "Update   590  TrnNLL  126.629  TstNLL   19.507  TrnA 0.963  TstA 0.931\n",
      "Update   595  TrnNLL  125.650  TstNLL   19.186  TrnA 0.965  TstA 0.922\n",
      "Update   600  TrnNLL  122.270  TstNLL   19.746  TrnA 0.971  TstA 0.931\n",
      "Update   605  TrnNLL  118.056  TstNLL   19.807  TrnA 0.967  TstA 0.931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update   610  TrnNLL  118.548  TstNLL   19.780  TrnA 0.968  TstA 0.931\n",
      "Update   615  TrnNLL  119.832  TstNLL   20.232  TrnA 0.968  TstA 0.931\n",
      "Update   620  TrnNLL  119.404  TstNLL   20.044  TrnA 0.969  TstA 0.931\n",
      "Update   625  TrnNLL  119.342  TstNLL   20.932  TrnA 0.970  TstA 0.931\n",
      "Update   630  TrnNLL  126.465  TstNLL   23.078  TrnA 0.964  TstA 0.914\n",
      "Update   635  TrnNLL  125.891  TstNLL   22.908  TrnA 0.965  TstA 0.914\n",
      "Update   640  TrnNLL  109.049  TstNLL   17.864  TrnA 0.975  TstA 0.948\n",
      "Update   645  TrnNLL  109.836  TstNLL   17.549  TrnA 0.976  TstA 0.948\n",
      "Update   650  TrnNLL  106.865  TstNLL   17.456  TrnA 0.976  TstA 0.948\n",
      "Update   655  TrnNLL  106.454  TstNLL   17.150  TrnA 0.977  TstA 0.948\n",
      "Update   660  TrnNLL  106.455  TstNLL   17.257  TrnA 0.975  TstA 0.948\n",
      "Update   665  TrnNLL  106.551  TstNLL   17.229  TrnA 0.975  TstA 0.948\n",
      "Update   670  TrnNLL  132.187  TstNLL   18.106  TrnA 0.964  TstA 0.948\n",
      "Update   675  TrnNLL  120.513  TstNLL   17.180  TrnA 0.969  TstA 0.948\n",
      "Update   680  TrnNLL  118.181  TstNLL   16.971  TrnA 0.971  TstA 0.948\n",
      "Update   685  TrnNLL  111.893  TstNLL   16.580  TrnA 0.973  TstA 0.948\n",
      "Update   690  TrnNLL  107.493  TstNLL   16.513  TrnA 0.975  TstA 0.948\n",
      "Update   695  TrnNLL  108.328  TstNLL   16.430  TrnA 0.976  TstA 0.948\n",
      "Update   700  TrnNLL  108.436  TstNLL   16.434  TrnA 0.976  TstA 0.948\n",
      "Update   705  TrnNLL  107.298  TstNLL   16.297  TrnA 0.976  TstA 0.948\n",
      "Update   710  TrnNLL  102.180  TstNLL   16.373  TrnA 0.976  TstA 0.940\n",
      "Update   715  TrnNLL  102.604  TstNLL   16.336  TrnA 0.977  TstA 0.940\n",
      "Update   720  TrnNLL  102.861  TstNLL   16.135  TrnA 0.976  TstA 0.948\n",
      "Update   725  TrnNLL  101.946  TstNLL   16.280  TrnA 0.975  TstA 0.940\n",
      "Update   730  TrnNLL   94.596  TstNLL   16.960  TrnA 0.975  TstA 0.922\n",
      "Update   735  TrnNLL   95.199  TstNLL   15.990  TrnA 0.976  TstA 0.948\n",
      "Update   740  TrnNLL   95.246  TstNLL   15.966  TrnA 0.976  TstA 0.948\n",
      "Update   745  TrnNLL   92.940  TstNLL   15.444  TrnA 0.978  TstA 0.940\n",
      "Update   750  TrnNLL   92.554  TstNLL   15.318  TrnA 0.978  TstA 0.948\n",
      "Update   755  TrnNLL   92.647  TstNLL   15.267  TrnA 0.978  TstA 0.948\n",
      "Update   760  TrnNLL  100.537  TstNLL   18.042  TrnA 0.976  TstA 0.914\n",
      "Update   765  TrnNLL  100.532  TstNLL   18.006  TrnA 0.975  TstA 0.914\n",
      "Update   770  TrnNLL   98.131  TstNLL   17.500  TrnA 0.977  TstA 0.914\n",
      "Update   775  TrnNLL  101.239  TstNLL   18.353  TrnA 0.975  TstA 0.914\n",
      "Update   780  TrnNLL   97.341  TstNLL   17.888  TrnA 0.976  TstA 0.914\n",
      "Update   785  TrnNLL  101.377  TstNLL   18.743  TrnA 0.972  TstA 0.914\n",
      "Update   790  TrnNLL  100.676  TstNLL   18.586  TrnA 0.973  TstA 0.914\n",
      "Update   795  TrnNLL   97.259  TstNLL   17.580  TrnA 0.976  TstA 0.922\n",
      "Update   800  TrnNLL  113.339  TstNLL   22.304  TrnA 0.967  TstA 0.922\n",
      "Update   805  TrnNLL  104.341  TstNLL   20.664  TrnA 0.970  TstA 0.922\n",
      "Update   810  TrnNLL   85.280  TstNLL   15.664  TrnA 0.979  TstA 0.940\n",
      "Update   815  TrnNLL   84.323  TstNLL   15.447  TrnA 0.979  TstA 0.940\n",
      "Update   820  TrnNLL   81.635  TstNLL   14.866  TrnA 0.981  TstA 0.948\n",
      "Update   825  TrnNLL   80.949  TstNLL   14.728  TrnA 0.980  TstA 0.948\n",
      "Update   830  TrnNLL   81.874  TstNLL   14.870  TrnA 0.981  TstA 0.948\n",
      "Update   835  TrnNLL   79.567  TstNLL   14.517  TrnA 0.980  TstA 0.948\n",
      "Update   840  TrnNLL  162.922  TstNLL   20.096  TrnA 0.964  TstA 0.948\n",
      "Update   845  TrnNLL  163.231  TstNLL   20.177  TrnA 0.964  TstA 0.948\n",
      "Update   850  TrnNLL  168.471  TstNLL   20.994  TrnA 0.962  TstA 0.940\n",
      "Update   855  TrnNLL  164.841  TstNLL   20.255  TrnA 0.964  TstA 0.940\n",
      "Update   860  TrnNLL  166.971  TstNLL   20.513  TrnA 0.962  TstA 0.948\n",
      "Update   865  TrnNLL  162.876  TstNLL   20.009  TrnA 0.966  TstA 0.940\n",
      "Update   870  TrnNLL  132.634  TstNLL   14.225  TrnA 0.971  TstA 0.957\n",
      "Update   875  TrnNLL  133.165  TstNLL   14.458  TrnA 0.970  TstA 0.948\n",
      "Update   880  TrnNLL  132.260  TstNLL   14.339  TrnA 0.971  TstA 0.948\n",
      "Update   885  TrnNLL  109.154  TstNLL   14.066  TrnA 0.969  TstA 0.948\n",
      "Update   890  TrnNLL  119.803  TstNLL   20.305  TrnA 0.966  TstA 0.940\n",
      "Update   895  TrnNLL  118.934  TstNLL   20.078  TrnA 0.966  TstA 0.940\n",
      "Update   900  TrnNLL  119.134  TstNLL   20.111  TrnA 0.966  TstA 0.940\n",
      "Update   905  TrnNLL  120.893  TstNLL   20.812  TrnA 0.964  TstA 0.931\n",
      "Update   910  TrnNLL  117.881  TstNLL   20.024  TrnA 0.965  TstA 0.940\n",
      "Update   915  TrnNLL  107.425  TstNLL   16.022  TrnA 0.968  TstA 0.948\n",
      "Update   920  TrnNLL  103.173  TstNLL   14.541  TrnA 0.975  TstA 0.940\n",
      "Update   925  TrnNLL  107.979  TstNLL   16.511  TrnA 0.971  TstA 0.948\n",
      "Update   930  TrnNLL  100.988  TstNLL   12.962  TrnA 0.977  TstA 0.948\n",
      "Update   935  TrnNLL   98.161  TstNLL   13.050  TrnA 0.980  TstA 0.948\n",
      "Update   940  TrnNLL   97.890  TstNLL   13.071  TrnA 0.980  TstA 0.948\n",
      "Update   945  TrnNLL   97.718  TstNLL   13.009  TrnA 0.980  TstA 0.948\n",
      "Update   950  TrnNLL   96.799  TstNLL   12.867  TrnA 0.980  TstA 0.948\n",
      "Update   955  TrnNLL   96.828  TstNLL   12.878  TrnA 0.980  TstA 0.948\n",
      "Update   960  TrnNLL   96.279  TstNLL   12.897  TrnA 0.980  TstA 0.948\n",
      "Update   965  TrnNLL   99.848  TstNLL   14.610  TrnA 0.977  TstA 0.940\n",
      "Update   970  TrnNLL   87.873  TstNLL   13.334  TrnA 0.981  TstA 0.948\n",
      "Update   975  TrnNLL   82.859  TstNLL   13.299  TrnA 0.984  TstA 0.948\n",
      "Update   980  TrnNLL   79.984  TstNLL   12.151  TrnA 0.987  TstA 0.948\n",
      "Update   985  TrnNLL   79.947  TstNLL   12.185  TrnA 0.987  TstA 0.948\n",
      "Update   990  TrnNLL   79.920  TstNLL   12.180  TrnA 0.987  TstA 0.948\n",
      "Update   995  TrnNLL   76.438  TstNLL   11.950  TrnA 0.985  TstA 0.957\n",
      "Update  1000  TrnNLL   75.412  TstNLL   11.826  TrnA 0.987  TstA 0.948\n",
      "Update  1005  TrnNLL   74.497  TstNLL   11.677  TrnA 0.987  TstA 0.948\n",
      "Update  1010  TrnNLL   72.690  TstNLL   12.998  TrnA 0.987  TstA 0.948\n",
      "Update  1015  TrnNLL   71.252  TstNLL   12.633  TrnA 0.988  TstA 0.948\n",
      "Update  1020  TrnNLL   71.089  TstNLL   12.453  TrnA 0.989  TstA 0.948\n",
      "Update  1025  TrnNLL   71.374  TstNLL   12.399  TrnA 0.987  TstA 0.948\n",
      "Update  1030  TrnNLL   71.258  TstNLL   12.394  TrnA 0.988  TstA 0.948\n",
      "Update  1035  TrnNLL   71.143  TstNLL   12.177  TrnA 0.989  TstA 0.948\n",
      "Update  1040  TrnNLL   69.941  TstNLL   12.186  TrnA 0.988  TstA 0.948\n",
      "Update  1045  TrnNLL   69.633  TstNLL   12.069  TrnA 0.990  TstA 0.948\n",
      "Update  1050  TrnNLL   69.007  TstNLL   11.963  TrnA 0.990  TstA 0.948\n",
      "Update  1055  TrnNLL   68.989  TstNLL   11.944  TrnA 0.990  TstA 0.948\n",
      "Update  1060  TrnNLL   70.150  TstNLL   12.780  TrnA 0.988  TstA 0.948\n",
      "Update  1065  TrnNLL   69.999  TstNLL   12.726  TrnA 0.988  TstA 0.948\n",
      "Update  1070  TrnNLL   69.886  TstNLL   12.708  TrnA 0.988  TstA 0.948\n"
     ]
    }
   ],
   "source": [
    "lr = LogReg(train_set, test_set, lam=0, eta=0.1)\n",
    "lr.train(isVerbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit tests are located in the script `tests.py` in this directory.  Execute the following cell to call the script and run the tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_unreg (__main__.TestLogReg) ... ok\n",
      "test_learnrate (__main__.TestLogReg) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.008s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your unregularized updates are working, modify the `sgd_update` function again to perform regularized updates using **Lazy Sparse Regularization**. Note that you should not regularize the bias weight. See the Lecture 4 in-class notebook for a refresher on LSR. **Note**: After implementing LSR, your code should still pass the unit tests for **Part A** when `lam = 0`. \n",
    "\n",
    "We've given you a third unit test in the next cell called `test_reg` based on the simple example of LSR worked out in  the Lecture 4 in-class notebook.  Do not move on to **Problem 3** until your code passes the test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_reg (__main__.TestLogReg) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 3: Hyperparameter Tuning \n",
    "***\n",
    "\n",
    "**Part A**: Perform a systematic study of the effect of the regularization parameter on the accuracy of your classifier on the test set.  Which choice of `lam` seems to do the best?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam_choices = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: For the value of `lam` chosen in **Part A** perform a systematic study of the choice of learning rate on the speed of convergence SGD.  Which learning rate seems to give the fastest convergence?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 4: Identifying Predictive and Non-Predictive Words \n",
    "***\n",
    "\n",
    "**Part A**: Find the top 10 words that are the best predictors for each class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Find the 10 words that are the worst predictors for class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
